\documentclass{acm_proc_article-sp}
\usepackage{verbatim}
\usepackage{graphicx}
\begin{document}

\title{Learning PSRs with The Base System}

\numberofauthors{1}

\author{
\alignauthor
Lucas Langer\\
\email{lucas.langer@mail.mcgill.ca}
}

\date{12 August 2015}

\maketitle

\section{Problem and Motivation}

% how to do subsections: \subsection{Another}

We consider the problem of learning models of time series data in partially observable environments. Typical applications arise in robotics and reinforcement learning, where hidden markov models (HMMs) are often the model of choice. We take particular interest in the class of partially observable systems which are compressible, namely systems in which one can achieve good performance with a smaller model. There exists a well known spectral algorithm for learning a Predictive State Representation of the environment from empirical data [NAME]. Traditionally, truncation of this learned PSR has been performed naively. We provide an extension to [NAME]'s algorithm for improved learning of smaller models. In empirical tests, our approach strongly outperforms it's ancestor in both in predictive and computational performance.

\section{Background and Related Work}

Predictive state representations are used as a model for predicting the probability of observations in a dynamical system. [NAME] gives an algorithm which makes use of Hankel matrices and a singular value decomposition to obtain a PSR from data. One can control the number of states in the PSR by including states with highest singular values. The reason for using less states is twofold. First, noise in empirical data artificial creates states with low singular values. Secondly, reducing the number of states is necessary in practice for computational performance. 

Previous work with PSR learning has included planning [pierre luc] and natural language processing [other source]. Our work focuses on improving the learning of PSRs for general applications.

\section{Approach and Uniqueness} 

In our work, we extend the standard PSR learning algorithm by developing a new machinery for performing queries which we call The Base System. This extra machinery allows us to  richly express state transitions with truncated models and provides better experimental performance. We first apply this system to timing applications where the construction of the Base System is easiest to standardize. We then progress to the general case of systems with multiple observations and develop a heuristic for constructing the Base System effectively from data.

\section{Results and Contributions}

\subsection{Preliminaries}
In the experiments that follow, we produce observations by simulating robot motion in labyrinth environments. We compare PSRs which use the base system to different degrees. To measure performance, the predictions of each PSR are compared to the actual probability distribution of the observations. 


\subsection{Double Loops}

In the first experiment we look at the time spent in double loop labyrinths. 

%pasted code to include images
\begin{figure}[ht!]
\centering
\includegraphics[width=45mm]{lucasplots/doubleloop0.png}
\caption{No noise \label{overflow}}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[width=45mm]{lucasplots/doubleloop0_1.png}
\caption{Corridor noise \label{overflow}}
\end{figure}

In both cases, the PSR with the Base System has 100 \% less error than without. In particular, we note that noise in the durations of loops doesn't harm the performance of the Base System.

\subsection{PacMan Labyrinth}

In the second experiment, we look at timing for a PacMan-Type labyrinth. In addition, we use state weightings from the learned PSRs to predict distances between the robot and objects in the environment. 

\begin{figure}[ht!]
\centering
\includegraphics[width=45mm]{lucasplots/pacman10000.png}
\caption{Timing predictions in Pacman \label{overflow}}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[width=45mm]{lucasplots/Distance.png}
\caption{Distance predictons from key \label{overflow}}
\end{figure}

Here, the Base System outperforms the naive by 100\% for timing and 45\% for distances.

\subsection{Multiple Observations}

Next, we change our set of observations to wall colors of the labyrinth. 

\begin{figure}[ht!]
\centering
\includegraphics[width=45mm]{lucasplots/MO.png}
\caption{Predicting wall colors \label{overflow}}
\end{figure}

Here, the Base system outperforms that naive approach by 55\%. For this environment, we construct the base system separately for each symbol. In general, one might want to use a heuristic to optimize the construction of the base.

\subsection{Relevance}
The spectral framework for learning in partially observable environments has better theoretical guarantees [REFERENCE] than non-spectral methods. In this work, we showed a way to significantly improve results in practical settings, that is when one wants a smaller model. In future work, we hope to see a theoretical analysis of the apparent improvement and optimization in how one constructs the Base System for the multiple observation case. 


\begin{comment}
This is \textit{italic}, \textbf{bold}, \textsc{small caps}, \textsf{sans serif}

Math is writen inside dollars: $a = b$, $a_k$, $b^s$, $a_k^j$, $a_{opt}^{\star}$ $\sin(2 \pi) = 0$, greek letters are intuitive $\alpha$, $\beta$, $\sigma$, $\Sigma$, $\Gamma$, and so on

Full line equations use 2 dollars
$$ \sum_{i=0}^{\infty} \gamma^i = \frac{1}{1-\gamma} $$

Other math symbols $a \in A \cap B \subseteq C \equiv X \sim Y \leq Z$
\end{comment}


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\appendix
%Appendix A
\section{Headings in Appendices}
The rules about hierarchical headings discussed above for
the body of the article are different in the appendices.
In the \textbf{appendix} environment, the command
\textbf{section} is used to
indicate the start of each Appendix, with alphabetic order
designation (i.e. the first is A, the second B, etc.) and
a title (if you include one).  So, if you need
hierarchical structure
\textit{within} an Appendix, start with \textbf{subsection} as the
highest level. Here is an outline of the body of this
document in Appendix-appropriate form:
\subsection{Introduction}
\subsection{The Body of the Paper}
\subsubsection{Type Changes and  Special Characters}
\subsubsection{Math Equations}
\paragraph{Inline (In-text) Equations}
\paragraph{Display Equations}
\subsubsection{Citations}
\subsubsection{Tables}
\subsubsection{Figures}
\subsubsection{Theorem-like Constructs}
\subsubsection*{A Caveat for the \TeX\ Expert}
\subsection{Conclusions}
\subsection{Acknowledgments}
\subsection{Additional Authors}
This section is inserted by \LaTeX; you do not insert it.
You just add the names and information in the
\texttt{{\char'134}additionalauthors} command at the start
of the document.
\subsection{References}
Generated by bibtex from your ~.bib file.  Run latex,
then bibtex, then latex twice (to resolve references)
to create the ~.bbl file.  Insert that ~.bbl file into
the .tex source file and comment out
the command \texttt{{\char'134}thebibliography}.
% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
\section{More Help for the Hardy}
The acm\_proc\_article-sp document class file itself is chock-full of succinct
and helpful comments.  If you consider yourself a moderately
experienced to expert user of \LaTeX, you may find reading
it useful but please remember not to change it.
\balancecolumns
% That's all folks!
\end{document}


\end{document}



